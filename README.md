ğŸ©º PlexusAI â€” The Multimodal AI Doctor
ğŸ’¡ Overview
PlexusAI is an experimental AI-powered multimodal medical assistant that listens, sees, and speaks like a real doctor.
It combines audio, vision, and language understanding into one seamless interface â€” built for learning and demonstration purposes only.
âš™ï¸ Core Features
ğŸ§  Multimodal Intelligence â€” Accepts both voice and image inputs.
ğŸ—£ï¸ Voice Understanding (Groq + Whisper) â€” Converts patient speech to text using Groqâ€™s Whisper large-v3 model, enabling fast and accurate transcription.
ğŸ‘ï¸ Visual Diagnosis (LLaMA Vision) â€” Analyzes uploaded medical images via Meta LLaMA-4 Vision (Scout 17B) to simulate a medical opinion.
ğŸ§ Doctor-Like Reasoning â€” Generates a natural, human-like diagnostic response with brief explanations and suggested remedies.
ğŸ”Š Voice Output (ElevenLabs) â€” Converts the AI doctorâ€™s response back into lifelike speech using ElevenLabs TTS.
Interactive Gradio UI â€” A simple yet elegant web interface that lets users talk, upload, and hear results instantly.
## Tech Stack
Component	Technology
Speech-to-Text	Groq API (Whisper Large v3)
Vision + Language Model	Meta LLaMA-4 Scout-17B-16E-Instruct
Text-to-Speech	ElevenLabs API
Interface	Gradio
Environment Management	Python + dotenv
ğŸš€ How It Works
ğŸ™ï¸ The user records their voice describing symptoms.
ğŸ©» The user uploads a medical image (e.g., skin rash, X-ray).
âš—ï¸ The model analyzes both inputs using Groq (for STT) and LLaMA-4 Vision (for image reasoning).
ğŸ©º PlexusAI generates a professional doctor-like response â€” short, conversational, and informative.
ğŸ”Š Finally, ElevenLabs TTS delivers the answer in natural voice.
